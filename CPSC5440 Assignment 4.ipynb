{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 155,
     "status": "ok",
     "timestamp": 1682080562747,
     "user": {
      "displayName": "Kyle Ford",
      "userId": "02444029364121372576"
     },
     "user_tz": 240
    },
    "id": "AVdFQJ0-wg4V",
    "outputId": "41a9c6cb-53f3-4dae-da03-428339a49379"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters:  144583\n",
      "Total vocab:  50\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#load ascii text and convert to lowercase\n",
    "filename = \"alice.txt\"\n",
    "raw_text = open(filename, 'r', encoding = 'utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "#create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c,i) for i, c in enumerate(chars))\n",
    "\n",
    "#summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print('Total characters: ', n_chars)\n",
    "print('Total vocab: ', n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2998,
     "status": "ok",
     "timestamp": 1682080567097,
     "user": {
      "displayName": "Kyle Ford",
      "userId": "02444029364121372576"
     },
     "user_tz": 240
    },
    "id": "leeUvzDkxgLb",
    "outputId": "be53813e-ca4b-4547-ef47-d78c8e6bc499"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patterns:  144483\n"
     ]
    }
   ],
   "source": [
    "#prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "  seq_in = raw_text[i:i+seq_length]\n",
    "  seq_out = raw_text[i+seq_length]\n",
    "  dataX.append([char_to_int[char] for char in seq_in])\n",
    "  dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5328,
     "status": "ok",
     "timestamp": 1682080574579,
     "user": {
      "displayName": "Kyle Ford",
      "userId": "02444029364121372576"
     },
     "user_tz": 240
    },
    "id": "hdgjgzHn1OZL",
    "outputId": "25853b25-ecb9-4349-faf1-3ea33cb0b252"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([144483, 100, 1]) torch.Size([144483])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "#reshape X to be [sampe, time steps, features]\n",
    "X = torch.tensor(dataX, dtype = torch.float32).reshape(n_patterns, seq_length, 1)\n",
    "X = X / float(n_vocab)\n",
    "y = torch.tensor(dataY)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1682080577358,
     "user": {
      "displayName": "Kyle Ford",
      "userId": "02444029364121372576"
     },
     "user_tz": 240
    },
    "id": "93F9a8hG2CLu"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "class CharModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.lstm = nn.LSTM(input_size = 1, hidden_size = 256, num_layers = 1, batch_first = True)\n",
    "    self.dropout = nn.Dropout(0.2)\n",
    "    self.linear = nn.Linear(256, n_vocab)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x, _ = self.lstm(x)\n",
    "    #take only the last output\n",
    "    x = x[:,-1,:]\n",
    "    #produce output\n",
    "    x=self.linear(self.dropout(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18031354,
     "status": "ok",
     "timestamp": 1682098610736,
     "user": {
      "displayName": "Kyle Ford",
      "userId": "02444029364121372576"
     },
     "user_tz": 240
    },
    "id": "oP8D_78E2rLe",
    "outputId": "3e869065-d0e7-4187-d59d-c314055e2d16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Cross-entropy:  409744.6562\n",
      "Epoch 1: Cross-entropy:  389397.0938\n",
      "Epoch 2: Cross-entropy:  375741.4375\n",
      "Epoch 3: Cross-entropy:  364444.0938\n",
      "Epoch 4: Cross-entropy:  354655.0312\n",
      "Epoch 5: Cross-entropy:  347332.9062\n",
      "Epoch 6: Cross-entropy:  337877.5000\n",
      "Epoch 7: Cross-entropy:  329361.4688\n",
      "Epoch 8: Cross-entropy:  323542.1875\n",
      "Epoch 9: Cross-entropy:  316755.9375\n",
      "Epoch 10: Cross-entropy:  310818.9688\n",
      "Epoch 11: Cross-entropy:  304734.7500\n",
      "Epoch 12: Cross-entropy:  298894.8125\n",
      "Epoch 13: Cross-entropy:  292670.8750\n",
      "Epoch 14: Cross-entropy:  286296.9688\n",
      "Epoch 15: Cross-entropy:  281316.7188\n",
      "Epoch 16: Cross-entropy:  276478.2500\n",
      "Epoch 17: Cross-entropy:  273235.2188\n",
      "Epoch 18: Cross-entropy:  267958.5312\n",
      "Epoch 19: Cross-entropy:  263817.1562\n"
     ]
    }
   ],
   "source": [
    "#Calculate loss and formulate best model. The model with the lowest loss will be used for text generation\n",
    "n_epochs = 20\n",
    "batch_size = 128\n",
    "model = CharModel()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "loader = data.DataLoader(data.TensorDataset(X,y), shuffle=True, batch_size = batch_size)\n",
    "\n",
    "best_model = None\n",
    "best_loss = np.inf\n",
    "for epoch in range(n_epochs):\n",
    "  model.train()\n",
    "  for X_batch, y_batch in loader:\n",
    "    y_pred = model(X_batch)\n",
    "    loss = loss_fn(y_pred, y_batch)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  #validation\n",
    "  model.eval()\n",
    "  loss = 0\n",
    "  with torch.no_grad():\n",
    "    for X_batch, y_batch in loader:\n",
    "      y_pred = model(X_batch)\n",
    "      loss += loss_fn(y_pred, y_batch)\n",
    "    if loss < best_loss:\n",
    "      best_loss = loss\n",
    "      best_model = model.state_dict()\n",
    "    print(\"Epoch %d: Cross-entropy: % .4f\" % (epoch, loss))\n",
    "\n",
    "torch.save([best_model, char_to_int], \"single-char.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1682098629092,
     "user": {
      "displayName": "Kyle Ford",
      "userId": "02444029364121372576"
     },
     "user_tz": 240
    },
    "id": "ga8-_GZ7BI8v"
   },
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "start = np.random.randint(0, len(raw_text)-seq_length)\n",
    "prompt = raw_text[start:start+seq_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4718,
     "status": "ok",
     "timestamp": 1682098794297,
     "user": {
      "displayName": "Kyle Ford",
      "userId": "02444029364121372576"
     },
     "user_tz": 240
    },
    "id": "LUarHkj1BLh2",
    "outputId": "6dde9d47-8024-4b7b-d550-57bbd3fadb1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: f\n",
      "_that_ is—‘take care of the sense, and the sounds will take care of\n",
      "themselves.’”\n",
      "\n",
      "“how fond she i\n",
      "ore toine iire t shene dane an ince,” said the macc tfrtree. \n",
      "“i toon to then io the sooe,” she macc thehe degan.\n",
      "“oo you moow that io the doerense tound ”ou solle to toe toinge io the siaee.”\n",
      "\n",
      "“i whsh i can _oeed ”hu was in ” said the macc tfrtree. \n",
      "“it woutd you know that ”ou cane doeng ” said the macc tfrtree. \n",
      "“i toon to then io the sooe,” she macc thehe degan.\n",
      "“oo you moow that io the doerense tound ”ou solle to toe toinge io the siaee.”\n",
      "\n",
      "“i whsh i can _oeed ”hu was in ” said the macc tfrtree. \n",
      "“it woutd you know that ”ou cane doeng ” said the macc tfrtree. \n",
      "“i toon to then io the sooe,” she macc thehe degan.\n",
      "“oo you moow that io the doerense tound ”ou solle to toe toinge io the siaee.”\n",
      "\n",
      "“i whsh i can _oeed ”hu was in ” said the macc tfrtree. \n",
      "“it woutd you know that ”ou cane doeng ” said the macc tfrtree. \n",
      "“i toon to then io the sooe,” she macc thehe degan.\n",
      "“oo you moow that io the doerense tound ”ou solle to toe toinge io the siaee.”\n",
      "\n",
      "“i whsh i can _oeed ”hu was in ” said the ma\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Basic LSTM to generate text\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "best_model, char_to_int = torch.load(\"single-char.pth\")\n",
    "n_vocab = len(char_to_int)\n",
    "int_to_char = dict((i,c) for c,i in char_to_int.items())\n",
    "\n",
    "#reload the model\n",
    "class CharModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=1, batch_first=True)\n",
    "    self.dropout= nn.Dropout(0.2)\n",
    "    self.linear = nn.Linear(256, n_vocab)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x, _ = self.lstm(x)\n",
    "    #take only the last output\n",
    "    x=x[:,-1,:]\n",
    "    #produce output\n",
    "    x=self.linear(self.dropout(x))\n",
    "    return x\n",
    "\n",
    "model = CharModel()\n",
    "model.load_state_dict(best_model)\n",
    "\n",
    "#randomly generate a prompt\n",
    "filename = \"alice.txt\"\n",
    "seq_length = 100\n",
    "raw_text = open(filename, 'r', encoding = 'utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "start = np.random.randint(0, len(raw_text)-seq_length)\n",
    "prompt=raw_text[start:start+seq_length]\n",
    "pattern=[char_to_int[c] for c in prompt]\n",
    "\n",
    "model.eval()\n",
    "print(\"Prompt: %s\" % prompt)\n",
    "with torch.no_grad():\n",
    "  for i in range(1000):\n",
    "    #format input array of int into PyTorch tensor\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)\n",
    "    x = torch.tensor(x, dtype=torch.float32)\n",
    "    #generate logits as output from the model\n",
    "    prediction = model(x)\n",
    "    #convert logits into one character\n",
    "    index = int(prediction.argmax())\n",
    "    result = int_to_char[index]\n",
    "    print(result, end=\"\")\n",
    "    #append the new character into the prompt for the next iteration\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:]\n",
    "print()\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMNKtqSGyqgu269aCGQ/TRV",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
